{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.2.0-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\ml notebooks\\basic practice\\ml_basic_practice_repo\\basic_env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\ml notebooks\\basic practice\\ml_basic_practice_repo\\basic_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading numpy-2.2.0-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.6 MB 2.8 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.5/12.6 MB 2.8 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.3/12.6 MB 1.8 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.6/12.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.1/12.6 MB 1.8 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 2.4/12.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 2.6/12.6 MB 1.8 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 2.9/12.6 MB 1.6 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 3.1/12.6 MB 1.7 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 3.7/12.6 MB 1.7 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.9/12.6 MB 1.7 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 4.5/12.6 MB 1.7 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 4.7/12.6 MB 1.7 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 4.7/12.6 MB 1.7 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 5.0/12.6 MB 1.6 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 5.2/12.6 MB 1.6 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.5/12.6 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 5.8/12.6 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 5.8/12.6 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 6.0/12.6 MB 1.4 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 6.0/12.6 MB 1.4 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 6.3/12.6 MB 1.4 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 6.3/12.6 MB 1.4 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 6.6/12.6 MB 1.3 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 6.6/12.6 MB 1.3 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 6.6/12.6 MB 1.3 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.8/12.6 MB 1.2 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.8/12.6 MB 1.2 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.8/12.6 MB 1.2 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.8/12.6 MB 1.2 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.8/12.6 MB 1.2 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.8/12.6 MB 1.2 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.8/12.6 MB 1.2 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.8/12.6 MB 1.2 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 7.1/12.6 MB 932.0 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 7.1/12.6 MB 932.0 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 7.1/12.6 MB 932.0 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 7.1/12.6 MB 932.0 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 7.1/12.6 MB 932.0 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 7.1/12.6 MB 932.0 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 7.1/12.6 MB 932.0 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 7.1/12.6 MB 932.0 kB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 7.3/12.6 MB 786.4 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 7.3/12.6 MB 786.4 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 7.3/12.6 MB 786.4 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 7.3/12.6 MB 786.4 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 7.3/12.6 MB 786.4 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 7.3/12.6 MB 786.4 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 7.3/12.6 MB 786.4 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 7.3/12.6 MB 786.4 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 7.6/12.6 MB 687.8 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 7.6/12.6 MB 687.8 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 7.6/12.6 MB 687.8 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 7.6/12.6 MB 687.8 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 7.6/12.6 MB 687.8 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 7.9/12.6 MB 640.2 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 7.9/12.6 MB 640.2 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 7.9/12.6 MB 640.2 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 7.9/12.6 MB 640.2 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 8.1/12.6 MB 625.2 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 8.1/12.6 MB 625.2 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 8.1/12.6 MB 625.2 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 8.4/12.6 MB 613.3 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 8.4/12.6 MB 613.3 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 8.4/12.6 MB 613.3 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 8.7/12.6 MB 607.3 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 8.7/12.6 MB 607.3 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 8.9/12.6 MB 607.1 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 9.2/12.6 MB 610.1 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 9.2/12.6 MB 610.1 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 9.4/12.6 MB 613.6 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 9.4/12.6 MB 613.6 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 9.7/12.6 MB 613.8 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 10.0/12.6 MB 617.7 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 10.0/12.6 MB 617.7 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 10.2/12.6 MB 623.8 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 10.5/12.6 MB 631.0 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 10.7/12.6 MB 635.5 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 10.7/12.6 MB 635.5 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 10.7/12.6 MB 635.5 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 11.0/12.6 MB 631.6 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 11.0/12.6 MB 631.6 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 11.3/12.6 MB 629.7 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 11.3/12.6 MB 629.7 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 11.5/12.6 MB 630.0 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 11.8/12.6 MB 633.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 11.8/12.6 MB 633.1 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 12.1/12.6 MB 636.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  12.3/12.6 MB 639.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  12.3/12.6 MB 639.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 640.5 kB/s eta 0:00:00\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, et-xmlfile, pandas, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 numpy-2.2.0 openpyxl-3.1.5 pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_excel(\"basic_odd_even_dataset.xlsx\")\n",
    "df_array=df.to_numpy()\n",
    "X=df_array[:,0]\n",
    "y=df_array[:,1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.reshape(-1, 1)\n",
    "y=y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=1\n",
    "hidden_size=2\n",
    "output_size=1\n",
    "\n",
    "\n",
    "#initialize weights and bias\n",
    "np.random.seed(42)\n",
    "W1=np.random.randn(input_size,hidden_size)*0.01\n",
    "b1=np.zeros((1,hidden_size))\n",
    "W2=np.random.randn(hidden_size,output_size)*0.01\n",
    "b2=np.zeros((1,output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation functions\n",
    "def relu(z):\n",
    "    return np.maximum(0,z)\n",
    "def relu_derivative(z):\n",
    "    return z>0\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "def sigmoid_derivative(A):\n",
    "    return A*(1-A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x,W1,b1,W2,b2):\n",
    "    Z1=np.dot(x,W1)+b1\n",
    "    A1=relu(Z1)\n",
    "    Z2=np.dot(A1,W2)+b2\n",
    "    A2=sigmoid(Z2)\n",
    "    return Z1,A1,Z2,A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_true,y_pred):\n",
    "    m=y_true.shape[0]\n",
    "    #we are using binary loss entropy as loss function\n",
    "    loss=-np.sum((y_true*np.log(y_pred))+((1-y_true)*np.log(1-y_pred)))/m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(X,y,Z1,A1,Z2,A2,W1,W2):\n",
    "    m=X.shape[0]\n",
    "\n",
    "    #output layer gradients\n",
    "    dz2=A2-y\n",
    "    dw2=(np.dot(A1.T,dz2))/m\n",
    "    db2=np.sum(dz2, axis=0, keepdims=True)/m\n",
    "\n",
    "    #hidden layer gradients\n",
    "    dA1=np.dot(dz2,W2.T)\n",
    "    dz1=dA1*relu_derivative(Z1)\n",
    "    dw1=(np.dot(X.T,dz1))/m\n",
    "    db1=np.sum(dz1, axis=0, keepdims=True)/m\n",
    "\n",
    "    return dw1,db1,dw2,db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights_and_bias(W1,b1,W2,b2,dw1,db1,dw2,db2,learning_rate):\n",
    "    W1=W1-(learning_rate*dw1)\n",
    "    b1=b1-(learning_rate*db1)\n",
    "    W2=W2-(learning_rate*dw2)\n",
    "    b2=b2-(learning_rate*db2)\n",
    "\n",
    "    return W1,b1,W2,b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial:  [[ 0.00496714 -0.00138264]] [[0. 0.]] [[0.00647689]\n",
      " [0.0152303 ]] [[0.]]\n",
      "gradients: [[0. 0.]] [[0. 0.]] [[0.]\n",
      " [0.]] [[0.]]\n",
      "[[ 0.00496714 -0.00138264]] [[0. 0.]] [[0.00647689]\n",
      " [0.0152303 ]] [[0.]]\n",
      "Epoch 0, Loss: 0.693139141345586\n",
      "gradients: [[0. 0.]] [[0. 0.]] [[0.]\n",
      " [0.]] [[0.]]\n",
      "[[ 0.00496714 -0.00138264]] [[0. 0.]] [[0.00647689]\n",
      " [0.0152303 ]] [[0.]]\n",
      "Epoch 1, Loss: 0.693139141345586\n",
      "gradients: [[0. 0.]] [[0. 0.]] [[0.]\n",
      " [0.]] [[0.]]\n",
      "[[ 0.00496714 -0.00138264]] [[0. 0.]] [[0.00647689]\n",
      " [0.0152303 ]] [[0.]]\n",
      "Epoch 2, Loss: 0.693139141345586\n",
      "gradients: [[0. 0.]] [[0. 0.]] [[0.]\n",
      " [0.]] [[0.]]\n",
      "[[ 0.00496714 -0.00138264]] [[0. 0.]] [[0.00647689]\n",
      " [0.0152303 ]] [[0.]]\n",
      "Epoch 3, Loss: 0.693139141345586\n",
      "gradients: [[0. 0.]] [[0. 0.]] [[0.]\n",
      " [0.]] [[0.]]\n",
      "[[ 0.00496714 -0.00138264]] [[0. 0.]] [[0.00647689]\n",
      " [0.0152303 ]] [[0.]]\n",
      "Epoch 4, Loss: 0.693139141345586\n",
      "gradients: [[0. 0.]] [[0. 0.]] [[0.]\n",
      " [0.]] [[0.]]\n",
      "[[ 0.00496714 -0.00138264]] [[0. 0.]] [[0.00647689]\n",
      " [0.0152303 ]] [[0.]]\n",
      "Epoch 5, Loss: 0.693139141345586\n",
      "gradients: [[0. 0.]] [[0. 0.]] [[0.]\n",
      " [0.]] [[0.]]\n",
      "[[ 0.00496714 -0.00138264]] [[0. 0.]] [[0.00647689]\n",
      " [0.0152303 ]] [[0.]]\n",
      "Epoch 6, Loss: 0.693139141345586\n",
      "gradients: [[0. 0.]] [[0. 0.]] [[0.]\n",
      " [0.]] [[0.]]\n",
      "[[ 0.00496714 -0.00138264]] [[0. 0.]] [[0.00647689]\n",
      " [0.0152303 ]] [[0.]]\n",
      "Epoch 7, Loss: 0.693139141345586\n",
      "gradients: [[0. 0.]] [[0. 0.]] [[0.]\n",
      " [0.]] [[0.]]\n",
      "[[ 0.00496714 -0.00138264]] [[0. 0.]] [[0.00647689]\n",
      " [0.0152303 ]] [[0.]]\n",
      "Epoch 8, Loss: 0.693139141345586\n",
      "gradients: [[0. 0.]] [[0. 0.]] [[0.]\n",
      " [0.]] [[0.]]\n",
      "[[ 0.00496714 -0.00138264]] [[0. 0.]] [[0.00647689]\n",
      " [0.0152303 ]] [[0.]]\n",
      "Epoch 9, Loss: 0.693139141345586\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "learning_rate=0.01\n",
    "epochs=10\n",
    "print(\"initial: \",W1,b1,W2,b2)\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    Z1,A1,Z2,A2=forward_pass(X,W1,b1,W2,b2)\n",
    "    # print(\"forward:\",A1,A2)\n",
    "    loss=compute_loss(y,A2)\n",
    "    dw1,db1,dw2,db2=back_propagation(X,A2,Z1,A1,Z2,A2,W1,W2)\n",
    "    print(\"gradients:\",dw1,db1,dw2,db2)\n",
    "    W1,b1,W2,b2=update_weights_and_bias(W1,b1,W2,b2,dw1,db1,dw2,db2,learning_rate)\n",
    "    # Print loss every 100 epochs\n",
    "    # if epoch % 100 == 0:\n",
    "    print(W1,b1,W2,b2)\n",
    "    print(f\"Epoch {epoch}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "def predict(X, W1, b1, W2, b2):\n",
    "    _, _, _, A2 = forward_pass(X, W1, b1, W2, b2)\n",
    "    return (A2 > 0.5).astype(int)\n",
    "\n",
    "predictions = predict(X, W1, b1, W2, b2)\n",
    "print(\"Predictions:\", predictions.flatten())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
